{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:18:01.878009700Z",
     "start_time": "2023-12-16T17:17:56.610093400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from EB_GAN import *\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 100\n",
    "N_WORKERS = 4\n",
    "HIDDEN_DIM = 100\n",
    "learning_rates = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "N_CLASS = 10\n",
    "OUTPUT_IMG_GAP = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:18:01.906559500Z",
     "start_time": "2023-12-16T17:18:01.885008900Z"
    }
   },
   "id": "f4494e1fbbad62e5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "train_set = dset.CIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=N_WORKERS\n",
    ")\n",
    "from torchvision import datasets\n",
    "transform = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "data_loader = DataLoader(\n",
    "    datasets.CIFAR10('data/cifar10', train=True, download=True, transform=transform),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:18:03.821679800Z",
     "start_time": "2023-12-16T17:18:01.908561Z"
    }
   },
   "id": "2da7c8dfff43e51d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU USED FOR TRAINING\n",
      "Generator(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=8192, bias=True)\n",
      "    (4): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (deconv): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (nv): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=32, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16384, bias=True)\n",
      "    (1): BatchNorm1d(16384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (deconv): Sequential(\n",
      "    (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    print(\"GPU USED FOR TRAINING\")\n",
    "else:\n",
    "    print(\"CPU USED FOR TRAINING\")\n",
    "\n",
    "# define the model\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# define the optimizers\n",
    "optimizer_gen = optim.Adam(generator.parameters(), lr=learning_rates, betas=betas)\n",
    "optimizer_dis = optim.Adam(discriminator.parameters(), lr=learning_rates, betas=betas)\n",
    "\n",
    "# define the loss function\n",
    "dis_loss = nn.MSELoss().to(device)\n",
    "\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "print(generator)\n",
    "print(discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:18:04.233086900Z",
     "start_time": "2023-12-16T17:18:03.829141500Z"
    }
   },
   "id": "46fb90e8d5bbccc6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def loss_pull_away(inp):\n",
    "    norm = torch.norm(inp, p=2, dim=1, keepdim=True)\n",
    "    normalized_embeddings = inp / norm\n",
    "    similarity = torch.matmul(normalized_embeddings, normalized_embeddings.t()).pow(2)\n",
    "    loss_pa = (torch.sum(similarity) - inp.size()[0]) / (inp.size()[0] * (inp.size()[0] - 1))\n",
    "    return loss_pa"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:18:04.257637900Z",
     "start_time": "2023-12-16T17:18:04.235086400Z"
    }
   },
   "id": "b18ebae1afaa7285"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training starts!\n",
      "====================================================================================================\n",
      "Epoch 0:\n",
      "Generator loss: 3.0165, Discriminator loss: 3.7604\n",
      "Epoch finished in 23.23s\n",
      "\n",
      "Epoch 1:\n",
      "Generator loss: 1.7803, Discriminator loss: 3.5090\n",
      "Epoch finished in 22.93s\n",
      "\n",
      "Epoch 2:\n",
      "Generator loss: 1.4770, Discriminator loss: 3.4036\n",
      "Epoch finished in 23.06s\n",
      "\n",
      "Epoch 3:\n",
      "Generator loss: 1.1844, Discriminator loss: 3.4333\n",
      "Epoch finished in 25.07s\n",
      "\n",
      "Epoch 4:\n",
      "Generator loss: 1.1017, Discriminator loss: 3.4591\n",
      "Epoch finished in 27.19s\n",
      "\n",
      "Epoch 5:\n",
      "Generator loss: 1.1121, Discriminator loss: 3.4336\n",
      "Epoch finished in 24.82s\n",
      "\n",
      "Epoch 6:\n",
      "Generator loss: 1.1676, Discriminator loss: 3.3754\n",
      "Epoch finished in 23.31s\n",
      "\n",
      "Epoch 7:\n",
      "Generator loss: 1.1947, Discriminator loss: 3.3405\n",
      "Epoch finished in 24.07s\n",
      "\n",
      "Epoch 8:\n",
      "Generator loss: 1.2895, Discriminator loss: 3.2512\n",
      "Epoch finished in 23.62s\n",
      "\n",
      "Epoch 9:\n",
      "Generator loss: 1.2716, Discriminator loss: 3.2558\n",
      "Epoch finished in 24.89s\n",
      "\n",
      "Epoch 10:\n",
      "Generator loss: 1.3517, Discriminator loss: 3.1731\n",
      "Epoch finished in 24.77s\n",
      "\n",
      "Epoch 11:\n",
      "Generator loss: 1.4089, Discriminator loss: 3.1087\n",
      "Epoch finished in 25.84s\n",
      "\n",
      "Epoch 12:\n",
      "Generator loss: 1.3663, Discriminator loss: 3.1474\n",
      "Epoch finished in 25.62s\n",
      "\n",
      "Epoch 13:\n",
      "Generator loss: 1.4250, Discriminator loss: 3.1097\n",
      "Epoch finished in 24.36s\n",
      "\n",
      "Epoch 14:\n",
      "Generator loss: 1.4314, Discriminator loss: 3.0828\n",
      "Epoch finished in 26.53s\n",
      "\n",
      "Epoch 15:\n",
      "Generator loss: 1.4337, Discriminator loss: 3.0822\n",
      "Epoch finished in 23.40s\n",
      "\n",
      "Epoch 16:\n",
      "Generator loss: 1.3683, Discriminator loss: 3.1331\n",
      "Epoch finished in 23.93s\n",
      "\n",
      "Epoch 17:\n",
      "Generator loss: 1.3476, Discriminator loss: 3.1508\n",
      "Epoch finished in 23.90s\n",
      "\n",
      "Epoch 18:\n",
      "Generator loss: 1.3884, Discriminator loss: 3.1137\n",
      "Epoch finished in 24.12s\n",
      "\n",
      "Epoch 19:\n",
      "Generator loss: 1.5025, Discriminator loss: 3.0109\n",
      "Epoch finished in 24.92s\n",
      "\n",
      "Epoch 20:\n",
      "Generator loss: 1.5015, Discriminator loss: 2.9968\n",
      "Epoch finished in 25.83s\n",
      "\n",
      "Epoch 21:\n",
      "Generator loss: 1.5232, Discriminator loss: 2.9734\n",
      "Epoch finished in 24.58s\n",
      "\n",
      "Epoch 22:\n",
      "Generator loss: 1.5016, Discriminator loss: 2.9922\n",
      "Epoch finished in 25.05s\n",
      "\n",
      "Epoch 23:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m lossG_record \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     18\u001B[0m lossD_record \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (inputs, targets) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_loader):\n\u001B[0;32m     20\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     21\u001B[0m     targets \u001B[38;5;241m=\u001B[39m targets\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mLongTensor)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001B[0m, in \u001B[0;36mCIFAR10.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    115\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img)\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 118\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    121\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001B[0m, in \u001B[0;36mNormalize.forward\u001B[1;34m(self, tensor)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, tensor: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    270\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m        Tensor: Normalized Tensor image.\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001B[0m, in \u001B[0;36mnormalize\u001B[1;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensor, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m    361\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg should be Tensor Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensor)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\model\\AC-GAN\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001B[0m, in \u001B[0;36mnormalize\u001B[1;34m(tensor, mean, std, inplace)\u001B[0m\n\u001B[0;32m    926\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m std\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    927\u001B[0m     std \u001B[38;5;241m=\u001B[39m std\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 928\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdiv_(std)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(\"==> Training starts!\")\n",
    "print(\"=\"*100)\n",
    "lossG_history = []\n",
    "lossD_history = []\n",
    "\n",
    "fixed_noise = None\n",
    "discriminator.train()\n",
    "real_ones = torch.ones((BATCH_SIZE, 1), device=device)\n",
    "fake_ones = torch.zeros((BATCH_SIZE, 1), device=device)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "    generator.train()\n",
    "\n",
    "    lossG_record = []\n",
    "    lossD_record = []\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs = inputs.float().to(device)\n",
    "        targets = targets.type(torch.LongTensor).to(device)\n",
    "        batch_size = inputs.shape[0]\n",
    "        # Train Generator\n",
    "        noise = torch.randn(batch_size, HIDDEN_DIM).to(device)\n",
    "        if fixed_noise is None:\n",
    "            fixed_noise = noise.clone().detach().to(device)\n",
    "\n",
    "        optimizer_dis.zero_grad()\n",
    "        real, _ = discriminator(inputs)\n",
    "        real_loss = dis_loss(real, inputs)\n",
    "\n",
    "        # give the generator\n",
    "        img = generator(noise)\n",
    "        fake, _ = discriminator(img)\n",
    "        fake_loss = dis_loss(fake, img.detach())\n",
    "\n",
    "        loss = real_loss + torch.clamp(4 - fake_loss, min=0)\n",
    "        lossD_record.append(loss.cpu().detach().numpy())\n",
    "        loss.backward()\n",
    "        optimizer_dis.step()\n",
    "\n",
    "        optimizer_gen.zero_grad()\n",
    "        img2 = generator(noise)\n",
    "        fake2, nv = discriminator(img2)\n",
    "\n",
    "        loss2 = dis_loss(fake2, img2.detach())\n",
    "        loss_all = loss2 + 10.0 * loss_pull_away(nv.view(batch_size, -1))\n",
    "        lossG_record.append(loss_all.cpu().detach().numpy())\n",
    "        loss_all.backward()\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        # output images\n",
    "        if batch_idx % OUTPUT_IMG_GAP == 0:\n",
    "            batches_done = i * len(train_loader) + batch_idx\n",
    "            generated_images = generator(fixed_noise).data[:50]\n",
    "            save_image(generated_images, os.path.join('./images', \"%d.png\" % batches_done), nrow=5, normalize=True)\n",
    "\n",
    "    avg_loss_g = np.sum(np.asarray(lossG_record)) / len(lossG_record)\n",
    "    avg_loss_d = np.sum(np.asarray(lossD_record)) / len(lossD_record)\n",
    "    print(\"Generator loss: %.4f, Discriminator loss: %.4f\"%(avg_loss_g, avg_loss_d))\n",
    "\n",
    "    lossG_history.append(avg_loss_g)\n",
    "    lossD_history.append(avg_loss_d)\n",
    "\n",
    "    # save the model checkpoint\n",
    "    torch.save(generator.state_dict(), os.path.join('./model', 'generator.pth'))\n",
    "    torch.save(discriminator.state_dict(), os.path.join('./model', 'discriminator.pth'))\n",
    "    print(f\"Epoch finished in {time.time() - epoch_start:.2f}s\")\n",
    "    print(\"\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f\"==> Optimization finished in {time.time() - start:.2f}s!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T17:27:33.626301Z",
     "start_time": "2023-12-16T17:18:04.247634500Z"
    }
   },
   "id": "d7350d6eb649747a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1e3fada4dd05db6e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
